# ISL-Detector
Inspired by some events in our school, I embarked on a computer vision project to develop a gesture recognition program that processes live camera feed, to accurately detect & classify gestures from the Indian Sign Language characters. Since I am a novice in computer vision & object detection algorithms that leverage machine learning, I used MediaPipe, a premier hand landmark detection framework, to train a custom gesture classification model on my own ISL dataset that I was able to curate by including some open source datasets available on the internet. The program reads individual frames at an interval of 2 seconds and processes it through our model to make a classification on the gesture (if one is detected) and then output this classified character on the screen. The output also keeps concating the characters to formulate a sentence (upto 100 characters) to make it more accessible for the user who is reading. As for next steps, the goal is to build a more complex dataset including gestures that involve hand movement & train a new model on this dataset.
